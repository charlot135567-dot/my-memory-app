# ===================================================================
# 0. å¥—ä»¶ & å…¨åŸŸå‡½å¼ï¼ˆä¸€å®šæ”¾æœ€é ‚ï¼‰
# ===================================================================
import streamlit as st
import subprocess, sys, os, datetime as dt, pandas as pd, io, json, re, tomli, tomli_w
from streamlit_calendar import calendar

# ========== é™¤éŒ¯æ¸¬è©¦ ==========
st.sidebar.markdown("## ğŸ”§ é™¤éŒ¯è³‡è¨Š")

api_key = os.getenv("GEMINI_API_KEY")

if api_key:
    st.sidebar.success("âœ… GEMINI_API_KEY å·²è¨­å®š")
    st.sidebar.write(f"é•·åº¦: {len(api_key)} å­—å…ƒ")
else:
    st.sidebar.error("âŒ GEMINI_API_KEY æœªè¨­å®š")
    st.sidebar.info("è«‹åˆ° Settings â†’ Secrets è¨­å®š")
    
# ---------- å…¨åŸŸå·¥å…·å‡½å¼ ----------
def save_analysis_result(result, input_text):
    if "analysis_history" not in st.session_state:
        st.session_state.analysis_history = []
    st.session_state.analysis_history.append({
        "date": dt.datetime.now().strftime("%Y-%m-%d %H:%M"),
        "input_preview": input_text[:50] + "..." if len(input_text) > 50 else input_text,
        "result": result
    })
    if len(st.session_state.analysis_history) > 10:
        st.session_state.analysis_history.pop(0)

def to_excel(result: dict) -> bytes:
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
        for sheet, key in [("Words", "words"), ("Phrases", "phrases"), ("Grammar", "grammar")]:
            if key in result and result[key]:
                pd.DataFrame(result[key]).to_excel(writer, sheet_name=sheet, index=False)
        stats = pd.DataFrame({
            "é …ç›®": ["ç¸½å­—å½™æ•¸", "ç¸½ç‰‡èªæ•¸", "æ–‡æ³•é»æ•¸", "åˆ†ææ—¥æœŸ"],
            "æ•¸å€¼": [
                len(result.get("words", [])),
                len(result.get("phrases", [])),
                len(result.get("grammar", [])),
                dt.date.today().strftime("%Y-%m-%d")
            ]
        })
        stats.to_excel(writer, sheet_name="çµ±è¨ˆ", index=False)
    buffer.seek(0)
    return buffer.getvalue()

# ===================================================================
# 1. å´é‚Šæ¬„ï¼ˆä¸€æ¬¡ 4 é€£çµï¼Œç„¡é‡è¤‡ï¼‰
# ===================================================================
with st.sidebar:
    st.divider()
    c1, c2 = st.columns(2)
    c1.link_button("âœ¨ Google AI", "https://gemini.google.com/ ")
    c2.link_button("ğŸ¤– Kimi K2",   "https://kimi.moonshot.cn/ ")
    c3, c4 = st.columns(2)
    c3.link_button("ESV Bible", "https://wd.bible/bible/gen.1.cunps?parallel=esv.klb.jcb ")
    c4.link_button("THSV11",    "https://www.bible.com/zh-TW/bible/174/GEN.1.THSV11 ")

# ===================================================================
# 2. é é¢é…ç½® & Session åˆå€¼ï¼ˆåªç•™å…¨åŸŸæœƒç”¨åˆ°çš„ï¼‰
# ===================================================================
st.set_page_config(layout="wide", page_title="Bible Study AI App 2026")

# é€™äº›è®Šæ•¸åªæœ‰ TAB2 æœƒç”¨åˆ°ï¼Œä½†ç‚ºäº†é¿å…å¾ŒçºŒ TAB å¼•ç”¨å‡ºéŒ¯ï¼Œå…ˆçµ¦ç©ºå€¼
if 'analysis_history' not in st.session_state: st.session_state.analysis_history = []

# ---------- CSS ----------
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Gamja+Flower&display=swap ');
.cute-korean { font-family: 'Gamja Flower', cursive; font-size: 20px; color: #FF8C00; text-align: center; }
.small-font { font-size: 13px; color: #555555; margin-top: 5px !important; }
.grammar-box-container {
    background-color: #f8f9fa; border-radius: 8px; padding: 12px;
    border-left: 5px solid #FF8C00; text-align: left; margin-top: 0px;
}
.fc-daygrid-day-frame:hover {background-color: #FFF3CD !important; cursor: pointer; transform: scale(1.03); transition: .2s}
.fc-daygrid-day-frame:active {transform: scale(0.98); background-color: #FFE69C !important}
</style>
""", unsafe_allow_html=True)

# ---------- åœ–ç‰‡ & ç¾æˆ TAB ----------
IMG_URLS = {
    "A": "https://raw.githubusercontent.com/charlot135567-dot/my-memory-app/main/183ebb183330643.Y3JvcCw4MDgsNjMyLDAsMA.jpg ",
    "B": "https://raw.githubusercontent.com/charlot135567-dot/my-memory-app/main/f364bd220887627.67cae1bd07457.jpg ",
    "C": "https://raw.githubusercontent.com/charlot135567-dot/my-memory-app/main/68254faebaafed9dafb41918f74c202e.jpg ",
    "M1": "https://raw.githubusercontent.com/charlot135567-dot/my-memory-app/main/Mashimaro1.jpg ",
    "M2": "https://raw.githubusercontent.com/charlot135567-dot/my-memory-app/main/Mashimaro2.jpg ",
    "M3": "https://raw.githubusercontent.com/charlot135567-dot/my-memory-app/main/Mashimaro3.jpg ",
    "M4": "https://raw.githubusercontent.com/charlot135567-dot/my-memory-app/main/Mashimaro4.jpg "
}
with st.sidebar:
    st.markdown('<p class="cute-korean">ë‹¹ì‹ ì€ í•˜ë‚˜ë‹˜ì˜ ì†Œì¤‘í•œ ë³´ë¬¼ì…ë‹ˆë‹¤</p>', unsafe_allow_html=True)
    st.image(IMG_URLS["M3"], width=250)
    st.divider()

tabs = st.tabs(["ğŸ  æ›¸æ¡Œ", "ğŸ““ ç­†è¨˜", "âœï¸ æŒ‘æˆ°", "ğŸ“‚ è³‡æ–™åº«"])

# ===================================================================
# 3. TAB1 â”€ æ›¸æ¡Œï¼ˆå–®ç´”ç¶“æ–‡èˆ‡ä¾‹å¥ï¼Œç„¡æœˆæ›†ï¼‰
# ===================================================================
with tabs[0]:
    col_content, col_m1 = st.columns([0.65, 0.35])
    with col_content:
        st.info("**Becoming** / ğŸ‡¯ğŸ‡µ ãµã•ã‚ã—ã„ | ğŸ‡°ğŸ‡· ì–´ìš¸ë¦¬ëŠ” | ğŸ‡¹ğŸ‡­ à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡ | ğŸ‡¨ğŸ‡³ ç›¸ç¨±")
        st.success("""
            ğŸŒŸ **Pro 17:07** Fine speech is not becoming to a fool; still less is false speech to a prince.   
            ğŸ‡¯ğŸ‡µ ã™ãã‚ŒãŸè¨€è‘‰ã¯æ„šã‹è€…ã«ã¯ãµã•ã‚ã—ããªã„ã€‚å½ã‚Šã®è¨€è‘‰ã¯å›ä¸»ã«ã¯ãªãŠã•ã‚‰ãµã•ã‚ã—ããªã„ã€‚   
            ğŸ‡¨ğŸ‡³ æ„šé ‘äººèªªç¾è¨€æœ¬ä¸ç›¸ç¨±ï¼Œä½•æ³å›ç‹èªªè¬Šè©±å‘¢ï¼Ÿ
            """, icon="ğŸ“–")
    with col_m1:
        st.markdown(f"""
            <div style="display:flex;flex-direction:column;justify-content:space-between;height:100%;min-height:250px;text-align:center;">
                <div style="flex-grow:1;display:flex;align-items:center;justify-content:center;">
                    <img src="{IMG_URLS['M1']}" style="width:200px;margin-bottom:10px;">
                </div>
                <div class="grammar-box-container" style="margin-top:auto;">
                    <p style="margin:2px 0;font-size:14px;font-weight:bold;color:#333;">æ™‚æ…‹: ç¾åœ¨ç°¡å–®å¼</p>
                    <p style="margin:2px 0;font-size:14px;font-weight:bold;color:#333;">æ ¸å¿ƒç‰‡èª:</p>
                    <ul style="margin:0;padding-left:18px;font-size:13px;line-height:1.4;color:#555;">
                        <li>Fine speech (å„ªç¾è¨€è¾­)</li>
                        <li>Becoming to (ç›¸ç¨±)</li>
                        <li>Still less (ä½•æ³)</li>
                        <li>False speech (è™›å‡è¨€è¾­)</li>
                    </ul>
                </div>
            </div>
        """, unsafe_allow_html=True)
    st.divider()
    st.markdown("### âœï¸ æ–‡æ³•é‹ç”¨ä¾‹å¥")
    cl1, cl2 = st.columns(2)
    with cl1:
        st.markdown("**Ex 1:** *Casual attire is not becoming to a CEO; still less is unprofessional language.* <p class='small-font'>ä¾¿æœå°åŸ·è¡Œé•·ä¸ç›¸ç¨±ï¼›æ›´ä¸ç”¨èªªä¸å°ˆæ¥­çš„è¨€èªäº†ã€‚</p>", unsafe_allow_html=True)
    with cl2:
        st.markdown("**Ex 2:** *Wealth is not becoming to a man without virtue; still less is power.* <p class='small-font'>è²¡å¯Œå°æ–¼ç„¡å¾·ä¹‹äººä¸ç›¸ç¨±ï¼›æ›´ä¸ç”¨èªªæ¬ŠåŠ›äº†ã€‚</p>", unsafe_allow_html=True)

# ===================================================================
# 4. TAB2 â”€ æœˆæ›†å¾…è¾¦ï¼ˆæŠ˜è¡·ç‰ˆï¼‰
# ===================================================================
with tabs[1]:
    import datetime as dt, re, os, json

    # ---------- 0. æª”æ¡ˆæŒä¹…åŒ–å·¥å…· ----------
    TODO_FILE = "todos.json"

    def load_todos():
        if os.path.exists(TODO_FILE):
            try:
                with open(TODO_FILE, "r", encoding="utf-8") as f:
                    return json.load(f)
            except:
                pass
        return {}

    def save_todos():
        cutoff = str(dt.date.today() - dt.timedelta(days=60))
        keys_to_remove = [k for k in st.session_state.todo.keys() if k < cutoff]
        for k in keys_to_remove:
            del st.session_state.todo[k]
        with open(TODO_FILE, "w", encoding="utf-8") as f:
            json.dump(st.session_state.todo, f, ensure_ascii=False, indent=2)

    # ---------- 1. åˆå€¼èˆ‡è‡ªå‹•è®€æª” ----------
    for key in ('cal_key','sel_date','show_del','del_target'):
        if key not in st.session_state:
            st.session_state[key] = 0 if key=='cal_key' else False if key=='show_del' else {}
    if 'todo' not in st.session_state:
        st.session_state.todo = load_todos()

    # å»ºç«‹æœªä¾†60å¤©ç©ºæ¸…å–®
    today = dt.date.today()
    for i in range(60):
        d = str(today + dt.timedelta(days=i))
        if d not in st.session_state.todo:
            st.session_state.todo[d] = []

    # ---------- 2. Emoji å·¥å…· ----------
    _EMOJI_RE = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251]+', flags=re.UNICODE)
    def first_emoji(text: str) -> str:
        m = _EMOJI_RE.search(text)
        return m.group(0) if m else ""
    def remove_emoji(text: str) -> str:
        return _EMOJI_RE.sub("", text).strip()

# ---------- 3. äº‹ä»¶ä¾†æºï¼ˆåªé¡¯ç¤ºçŸ­æ¨™é¡Œï¼‰ ----------
def build_events():
    ev = []
    for d, todos in st.session_state.todo.items():
        if not isinstance(todos, list): 
            continue
        
        todos_sorted = sorted(todos, key=lambda x: x.get('time','00:00'))
        
        for t in todos_sorted:
            time_str = t.get('time','00:00:00')

            # â­ æœˆæ›†åªé¡¯ç¤ºçŸ­æ–‡å­—ï¼ˆé¿å…æ ¼å­çˆ†æ‰ï¼‰
            short_title = f"{t.get('emoji','ğŸ””')} {t['title']}"
            if len(short_title) > 20:
                short_title = short_title[:20] + "â€¦"

            start_iso = f"{d}T{time_str}"

            ev.append({
                "title": short_title,   # â† åªçµ¦çŸ­æ¨™é¡Œ
                "start": start_iso,
                "allDay": False,
                "backgroundColor": "#FFE4E1", 
                "borderColor": "#FFE4E1", 
                "textColor": "#333",

                # ä¿ç•™å®Œæ•´è³‡æ–™çµ¦ä¸‹æ–¹åˆ—è¡¨ç”¨ï¼ˆä¸å‹•çµæ§‹ï¼‰
                "extendedProps": {
                    "type": "todo", 
                    "date": d, 
                    "title": t['title'],
                    "time": time_str,
                    "emoji": t.get("emoji","ğŸ””")
                }
            })
    return ev

    # ---------- 4. CSS ç¾åŒ–ï¼ˆåªæ”¹æ–‡å­—æ›è¡Œï¼‰ ----------
    st.markdown("""
    <style>
    .fc-toolbar-title { font-size: 26px; font-weight: 700; color: #3b82f6; letter-spacing: 1px; }
    .fc-day-sat .fc-daygrid-day-number,
    .fc-day-sun .fc-daygrid-day-number { color: #dc2626 !important; font-weight: 600; }
    .fc-event { cursor: pointer; border: none; }
    .fc-event-title { white-space: normal !important; font-size:14px; line-height:1.4; }
    </style>
    """, unsafe_allow_html=True)

    # ---------- 5. æœˆæ›† ----------
    st.subheader("ğŸ“… æœˆæ›†å¾…è¾¦")
    with st.expander("å±•é–‹ / æŠ˜ç–Šæœˆæ›†è¦–çª—", expanded=True):
        calendar_events = build_events()
        calendar_options = {
            "headerToolbar":{"left":"prev,next today","center":"title","right":""},
            "initialView":"dayGridMonth",
            "height":"auto",
            "dateClick": True,
            "eventClick": True,
            "eventDisplay":"block",
            "eventTimeFormat":{"hour":"2-digit","minute":"2-digit","meridiem":False,"hour12":False}
        }
        state = calendar(events=calendar_events, options=calendar_options, key=f"emoji_cal_{st.session_state.cal_key}")

        # é»æ“Šäº‹ä»¶ â†’ å½ˆçª—åˆªé™¤
        if state.get("eventClick"):
            ext = state["eventClick"]["event"]["extendedProps"]
            if ext.get("type")=="todo":
                st.session_state.del_target = ext
                st.session_state.show_del = True
                st.rerun()

        # é»æ“Šæ—¥æœŸ â†’ é¸æ“‡æ—¥æœŸ
        if state.get("dateClick"):
            new_date = state["dateClick"]["date"][:10]
            if st.session_state.sel_date != new_date:
                st.session_state.sel_date = new_date
                st.rerun()

    # ---------- 6. åˆªé™¤å°è©±æ¡† ----------
    if st.session_state.get("show_del"):
        t = st.session_state.del_target
        st.warning(f"ğŸ—‘ï¸ ç¢ºå®šåˆªé™¤å¾…è¾¦ã€Œ{t.get('title','')}ã€ï¼Ÿ")
        c1,c2 = st.columns([1,1])
        with c1:
            if st.button("ç¢ºèªåˆªé™¤", key="confirm_del"):
                d = t.get("date")
                title_to_del = t.get("title")
                time_to_del = t.get("time")
                if d in st.session_state.todo:
                    st.session_state.todo[d] = [
                        item for item in st.session_state.todo[d]
                        if not (item['title']==title_to_del and item.get('time')==time_to_del)
                    ]
                    if not st.session_state.todo[d]: del st.session_state.todo[d]
                save_todos()
                st.session_state.show_del = False
                st.session_state.cal_key += 1
                st.success("âœ… å·²åˆªé™¤ï¼")
                st.rerun()
        with c2:
            if st.button("å–æ¶ˆ", key="cancel_del"):
                st.session_state.show_del = False
                st.rerun()

    # ---------- 7. ä¸‹æ–¹åˆ—è¡¨ ----------
    try:
        base_date = dt.datetime.strptime(st.session_state.sel_date, "%Y-%m-%d").date()
    except:
        base_date = dt.date.today()
    st.markdown("##### ğŸ“‹ è©³ç´°åˆ—è¡¨")
    has_items = False
    for i in range(3):
        dd = base_date + dt.timedelta(days=i)
        ds = str(dd)
        if ds in st.session_state.todo and st.session_state.todo[ds]:
            has_items = True
            date_display = f"{dd.month}/{dd.day}"
            sorted_items = sorted(st.session_state.todo[ds], key=lambda x:x.get('time','00:00'))
            for t in sorted_items:
                time_display = t.get('time','00:00')[:5]
                st.write(f"**{date_display} {time_display}** {t.get('emoji','ğŸ””')}{t['title']}")
    if not has_items:
        st.caption("æ­¤æœŸé–“å°šç„¡å¾…è¾¦äº‹é …")

    # ---------- 8. æ–°å¢å¾…è¾¦ ----------
    st.divider()
    with st.expander("â• æ–°å¢å¾…è¾¦", expanded=True):
        ph_emo = "ğŸ””"
        with st.form("todo_form"):
            try:
                default_date = dt.datetime.strptime(st.session_state.sel_date,"%Y-%m-%d").date()
            except:
                default_date = dt.date.today()
            c1,c2,c3 = st.columns([2,2,6])
            with c1: d_input = st.date_input("æ—¥æœŸ", default_date, label_visibility="collapsed", key="todo_date")
            with c2: tm_input = st.time_input("â° æ™‚é–“", dt.time(9,0), label_visibility="collapsed", key="todo_time")
            with c3: ttl_input = st.text_input("æ¨™é¡Œ", placeholder=f"{ph_emo} Emojiï¼‹å¾…è¾¦", label_visibility="collapsed", key="todo_ttl")
            submitted = st.form_submit_button("ğŸ’¾ å„²å­˜", use_container_width=True)
            if submitted:
                if not ttl_input:
                    st.error("è«‹è¼¸å…¥æ¨™é¡Œ")
                else:
                    emo_found = first_emoji(ttl_input) or ph_emo
                    ttl_clean = remove_emoji(ttl_input)
                    k = str(d_input)
                    if k not in st.session_state.todo: st.session_state.todo[k] = []
                    st.session_state.todo[k].append({
                        "title": ttl_clean, "time": str(tm_input), "emoji": emo_found
                    })
                    save_todos()
                    st.session_state.cal_key += 1
                    st.success("âœ… å·²å„²å­˜ï¼")
                    st.rerun()

# ===================================================================
# 5. TAB3 â”€ æŒ‘æˆ°ï¼ˆå–®ç´”ç¿»è­¯é¡Œï¼Œç„¡æœˆæ›†ï¼‰
# ===================================================================
with tabs[2]:
    col_challenge, col_deco = st.columns([0.7, 0.3])
    with col_challenge:
        st.subheader("ğŸ“ ç¿»è­¯æŒ‘æˆ°")
        st.write("é¡Œç›® 1: æ„šé ‘äººèªªç¾è¨€æœ¬ä¸ç›¸ç¨±...")
        st.text_input("è«‹è¼¸å…¥è‹±æ–‡ç¿»è­¯", key="ans_1_final", placeholder="Type your translation here...")
    with col_deco:
        st.image(IMG_URLS.get("B"), width=150, caption="Keep Going!")

# ===================================================================
# 5. TAB4 â”€ AI è–ç¶“åˆ†ææ§åˆ¶å°ï¼ˆå®Œæ•´ç‰ˆï¼‰
# ===================================================================
with tabs[3]:
    import os, json, datetime as dt, subprocess, sys, re
    import pandas as pd
    import streamlit as st

    # ============================================================
    # 0. è¨­å®šèˆ‡åˆå§‹åŒ–
    # ============================================================
    SENTENCES_FILE = "sentences.json"
    PROMPTS_FILE = "prompts_tab4.json"  # å…§å»º promptsï¼Œé¿å…æª”æ¡ˆéºå¤±å•é¡Œ
    
    # å…§å»º AI Promptsï¼ˆé¿å… Prompts.toml æ‰¾ä¸åˆ°çš„å•é¡Œï¼‰
    BUILTIN_PROMPTS = {
        "chinese_verse": """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è–ç¶“èªè¨€å­¸å®¶ã€‚è«‹é‡å°ä»¥ä¸‹ä¸­æ–‡è–ç¶“ç¶“æ–‡ï¼Œç”¢ç”Ÿçµæ§‹åŒ–å­¸ç¿’è³‡æ–™ã€‚

ç¶“æ–‡ï¼š{text}

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼å›å‚³ï¼ˆä¸è¦åŠ  markdown æ¨™è¨˜ï¼‰ï¼š

{{
  "ref_no": "è–ç¶“ç¸®å¯«+ç« ç¯€ï¼ˆä¾‹ï¼š2Ti 4:17-18ï¼‰",
  "ref_article": "è‹±æ–‡ç¶“æ–‡ï¼ˆESVç‰ˆæœ¬ï¼‰",
  "ref_article_zh": "è¼¸å…¥çš„ä¸­æ–‡ç¶“æ–‡",
  
  "v1_data": {{
    "Ref": "è–ç¶“ç¸®å¯«ç« ç¯€",
    "English": "ESVè‹±æ–‡ç¶“æ–‡",
    "Chinese": "ä¸­æ–‡ç¶“æ–‡",
    "Syn_Ant": "é«˜ç´šå–®å­—/ç‰‡èªçš„åŒç¾©åç¾©ï¼ˆä¸­è‹±å°ç…§ï¼‰",
    "Grammar": "æ–‡æ³•åˆ†æï¼ˆå«è£œé½Šå¥ã€æ‡‰ç”¨ä¾‹å¥ï¼‰"
  }},
  
  "v2_data": {{
    "Ref": "è–ç¶“ç¸®å¯«ç« ç¯€",
    "å£èªè¨³": "æ—¥æ–‡ç¶“æ–‡ï¼ˆå£èªé«”ï¼‰",
    "Grammar": "æ—¥æ–‡æ–‡æ³•è§£æ",
    "Note": "æ–‡æ³•è£œå……èªªæ˜",
    "KRF": "éŸ“æ–‡ç¶“æ–‡",
    "Syn_Ant_KR": "éŸ“æ–‡é«˜ç´šå­—å½™åŒç¾©åç¾©",
    "THSV11": "æ³°æ–‡é‡è¦ç‰‡èª"
  }},
  
  "words": [
    {{
      "Vocab": "è‹±æ–‡å–®å­—",
      "Syn_Ant": "åŒç¾©/åç¾©",
      "Example": "ç¶“æ–‡ä¸­çš„ä¾‹å¥",
      "å£èªè¨³": "æ—¥æ–‡ç¿»è­¯",
      "KRF": "éŸ“æ–‡ç¿»è­¯", 
      "THSV11": "æ³°æ–‡ç¿»è­¯"
    }}
  ],
  
  "phrases": [
    {{
      "Phrase": "è‹±æ–‡ç‰‡èª",
      "Syn_Ant": "åŒç¾©/åç¾©",
      "Example": "ç¶“æ–‡ä¸­çš„ä¾‹å¥",
      "å£èªè¨³": "æ—¥æ–‡ç¿»è­¯",
      "KRF": "éŸ“æ–‡ç¿»è­¯",
      "THSV11": "æ³°æ–‡ç¿»è­¯"
    }}
  ],
  
  "grammar": [
    {{
      "Rule": "æ–‡æ³•è¦å‰‡åç¨±",
      "Example": "åŸæ–‡ä¾‹å¥",
      "è§£æ": "ä¸­æ–‡æ–‡æ³•è§£æ",
      "è£œé½Šå¥": "è£œå……å®Œæ•´å¥å­",
      "æ‡‰ç”¨ä¾‹": "ä¸­è‹±å°ç…§æ‡‰ç”¨ä¾‹å¥"
    }}
  ]
}}""",

        "english_manuscript": """è§’è‰²ï¼šä½ æ˜¯ä¸€ä½ç²¾é€šèªè¨€å­¸èˆ‡è–ç¶“è§£ç¶“çš„æ•™æç·¨è¼¯ã€‚
ç›®æ¨™ï¼šå°‡ã€Œå£èªè¬›é“é€å­—ç¨¿ã€è½‰åŒ–ç‚ºã€Œç²¾ç…‰çš„é›™èªè–ç¶“å­¸ç¿’æ•™æã€ã€‚

è«‹é‡å°ä»¥ä¸‹è¬›ç¨¿ï¼Œç”¢å‡ºçµæ§‹åŒ–å­¸ç¿’æ•¸æ“šï¼š

{text}

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼å›å‚³ï¼ˆä¸è¦åŠ  markdown æ¨™è¨˜ï¼‰ï¼š

{{
  "ref_no": "è¬›ç¨¿ç·¨è™Ÿï¼ˆæ—¥æœŸ+åºè™Ÿï¼Œä¾‹ï¼š2025012701ï¼‰",
  "ref_article": "ç´”è‹±æ–‡ç²¾ç…‰ç¨¿ï¼ˆOutline 1-5ï¼Œå»é™¤å£æ°£è©ï¼Œé«˜ç´šè©å½™åŠ ç²—ä¸¦é™„ä¸­æ–‡è§£é‡‹ï¼‰",
  "ref_article_zh": "ä¸­è‹±å¤¾é›œè¬›ç« ï¼ˆèˆ‡è‹±æ–‡ç‰ˆåŒæ­¥ï¼Œè‹±æ–‡è©å½™åµŒå…¥æ‹¬è™Ÿå°ç…§ï¼‰",
  
  "words": [
    {{
      "Vocab": "é«˜ç´š/ä¸­é«˜ç´šå–®å­—",
      "Syn_Ant": "åŒç¾©/åç¾©ï¼ˆä¸­è‹±ï¼‰",
      "Example": "è¬›ç¨¿ä¸­çš„ä¾‹å¥",
      "å£èªè¨³": "æ—¥æ–‡ç¿»è­¯",
      "KRF": "éŸ“æ–‡ç¿»è­¯",
      "THSV11": "æ³°æ–‡ç¿»è­¯"
    }}
  ],
  
  "phrases": [
    {{
      "Phrase": "é«˜ç´š/ä¸­é«˜ç´šç‰‡èª", 
      "Syn_Ant": "åŒç¾©/åç¾©ï¼ˆä¸­è‹±ï¼‰",
      "Example": "è¬›ç¨¿ä¸­çš„ä¾‹å¥",
      "å£èªè¨³": "æ—¥æ–‡ç¿»è­¯",
      "KRF": "éŸ“æ–‡ç¿»è­¯",
      "THSV11": "æ³°æ–‡ç¿»è­¯"
    }}
  ],
  
  "grammar": [
    {{
      "Rule": "æ–‡æ³•è¦å‰‡åç¨±",
      "Example": "åŸç¨¿ç¯„ä¾‹",
      "è§£æ": "ä¸­æ–‡æ–‡æ³•è§£æ",
      "è£œé½Šå¥": "è£œé½Šå¾Œçš„å®Œæ•´å¥",
      "æ‡‰ç”¨ä¾‹": "ä¸­è‹±å°ç…§æ‡‰ç”¨ä¾‹å¥"
    }}
  ]
}}

æ ¼å¼è¦æ±‚ï¼š
1. ç´”è‹±æ–‡ç²¾ç…‰ç¨¿èˆ‡ä¸­è‹±å¤¾é›œè¬›ç« è¦**äº¤éŒ¯å‘ˆç¾**ï¼ˆä¸€æ®µè‹±æ–‡é…ä¸€æ®µä¸­è‹±ï¼‰
2. ç¦æ­¢ä½¿ç”¨ HTML æ¨™ç±¤ï¼Œåªç”¨ Markdown åŠ ç²—
3. æ®µè½é–“è¦æœ‰ç©ºè¡Œ
4. ç¿»è­¯å¿…é ˆå°ç…§è–ç¶“åŸæ–‡ï¼Œç¦æ­¢è‡ªè¡Œäº‚ç¿»"""
    }

    def load_sentences():
        """è¼‰å…¥è³‡æ–™åº«"""
        if os.path.exists(SENTENCES_FILE):
            try:
                with open(SENTENCES_FILE, "r", encoding="utf-8") as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def save_sentences():
        """å„²å­˜è³‡æ–™åº«"""
        try:
            with open(SENTENCES_FILE, "w", encoding="utf-8") as f:
                json.dump(st.session_state.sentences, f, ensure_ascii=False, indent=2)
        except Exception as e:
            st.error(f"å­˜æª”å¤±æ•—ï¼š{e}")

    def save_analysis_result(data, input_text, analysis_type):
        """å„²å­˜åˆ†ææ­·å²"""
        if "analysis_history" not in st.session_state:
            st.session_state.analysis_history = []
        
        record = {
            "timestamp": dt.datetime.now().strftime("%Y-%m-%d %H:%M"),
            "type": analysis_type,  # "chinese_verse" æˆ– "english_manuscript"
            "ref_no": data.get("ref_no", ""),
            "input": input_text[:150] + "..." if len(input_text) > 150 else input_text,
            "data": data
        }
        st.session_state.analysis_history.append(record)

    # ============================================================
    # 1. åˆå§‹åŒ– Session State
    # ============================================================
    if 'sentences' not in st.session_state:
        st.session_state.sentences = load_sentences()
    if 'show_result' not in st.session_state:
        st.session_state.show_result = False
    if 'current_analysis' not in st.session_state:
        st.session_state.current_analysis = None

    # ============================================================
    # 2. AI åˆ†æå‡½æ•¸
    # ============================================================
    def detect_language(text):
        """åµæ¸¬è¼¸å…¥èªè¨€"""
        chinese_chars = sum(1 for c in text[:200] if '\u4e00' <= c <= '\u9fff')
        return "chinese" if chinese_chars > 10 else "english"

    def call_gemini_api(prompt, api_key):
        """å‘¼å« Gemini API"""
        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-pro')
            
            response = model.generate_content(
                prompt,
                generation_config={
                    'temperature': 0.2,
                    'max_output_tokens': 8192,
                }
            )
            
            # æ¸…ç†å›æ‡‰
            text = response.text
            text = re.sub(r'```json\s*', '', text)
            text = re.sub(r'```\s*', '', text)
            return text.strip()
            
        except Exception as e:
            st.error(f"Gemini API éŒ¯èª¤ï¼š{e}")
            return None

    def analyze_with_ai(text, analysis_type, api_key):
        """åŸ·è¡Œ AI åˆ†æ"""
        prompt_template = BUILTIN_PROMPTS.get(analysis_type, BUILTIN_PROMPTS["english_manuscript"])
        prompt = prompt_template.format(text=text[:4000])  # é™åˆ¶é•·åº¦
        
        response_text = call_gemini_api(prompt, api_key)
        
        if not response_text:
            return None
        
        try:
            data = json.loads(response_text)
            
            # ç¢ºä¿å¿…è¦æ¬„ä½å­˜åœ¨
            required_fields = ["ref_no", "ref_article", "words", "phrases", "grammar"]
            for field in required_fields:
                if field not in data:
                    if field in ["words", "phrases", "grammar"]:
                        data[field] = []
                    else:
                        data[field] = ""
            
            # åŠ å…¥æ™‚é–“æˆ³
            data["analyzed_at"] = dt.datetime.now().isoformat()
            data["input_type"] = analysis_type
            
            return data
            
        except json.JSONDecodeError as e:
            st.error(f"JSON è§£æéŒ¯èª¤ï¼š{e}")
            with st.expander("æŸ¥çœ‹åŸå§‹å›æ‡‰"):
                st.code(response_text[:1000])
            return None

    def create_fallback_data(text, analysis_type):
        """AI å¤±æ•—æ™‚çš„å›é€€è³‡æ–™"""
        ref_no = f"FB{dt.datetime.now().strftime('%Y%m%d%H%M')}"
        
        is_chinese = analysis_type == "chinese_verse"
        
        return {
            "ref_no": ref_no,
            "ref_article": text[:300] + "..." if len(text) > 300 else text,
            "ref_article_zh": "ï¼ˆâš ï¸ AI åˆ†æå¤±æ•—ï¼Œé¡¯ç¤ºé è¨­è³‡æ–™ï¼‰" if is_chinese else "",
            "input_type": analysis_type,
            "analyzed_at": dt.datetime.now().isoformat(),
            "words": [
                {"Vocab": "becoming", "Syn_Ant": "fitting / unbecoming", "Example": "Fine speech is not becoming to a fool.", "å£èªè¨³": "æ„šã‹è€…ã«ã¯ãµã•ã‚ã—ããªã„", "KRF": "ì–´ìš¸ë¦¬ì§€ ì•ŠëŠ”ë‹¤", "THSV11": "à¹„à¸¡à¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡"},
                {"Vocab": "rescue", "Syn_Ant": "save / abandon", "Example": "The Lord will rescue me.", "å£èªè¨³": "æ•‘ã„å‡ºã™", "KRF": "êµ¬ì¶œí•˜ë‹¤", "THSV11": "à¸Šà¹ˆà¸§à¸¢à¹ƒà¸«à¹‰à¸à¹‰à¸™"},
            ],
            "phrases": [
                {"Phrase": "fine speech", "Syn_Ant": "eloquent words", "Example": "Fine speech is not becoming to a fool.", "å£èªè¨³": "ç¾è¾éº—å¥", "KRF": "ì•„ë¦„ë‹¤ìš´ ë§", "THSV11": "à¸§à¸²à¸ˆà¸²à¸‡à¸²à¸¡"},
            ],
            "grammar": [
                {"Rule": "becoming to + N", "Example": "Fine speech is not becoming to a fool.", "è§£æ": "ã€ç›¸ç¨±ã€ç¾©å½¢å®¹è©ç‰‡èª", "è£œé½Šå¥": "Honesty is becoming to a leader.", "æ‡‰ç”¨ä¾‹": "Humility is becoming to us."},
            ],
            "is_fallback": True
        }

    # ============================================================
    # 3. UI ä»‹é¢
    # ============================================================
    
    # æª¢æŸ¥ API Key
    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("KIMI_API_KEY")
    
    st.markdown("## ğŸ¤– AI è–ç¶“åˆ†ææ§åˆ¶å°")
    
    if not api_key:
        st.warning("âš ï¸ å°šæœªè¨­å®š GEMINI_API_KEYï¼Œè«‹åœ¨ Streamlit Secrets è¨­å®šå¾Œé‡æ–°å•Ÿå‹•")
    
    # ä¸»æ“ä½œå€
    with st.expander("ğŸ“š åˆ†æè¨­å®š", expanded=True):
        
        # è¼¸å…¥å€
        input_text = st.text_area(
            "è²¼ä¸Šç¶“æ–‡æˆ–è¬›ç¨¿ï¼ˆæ”¯æ´ä¸­æ–‡ç¶“æ–‡æˆ–è‹±æ–‡è¬›ç¨¿ï¼‰",
            height=250,
            key="tab4_input",
            placeholder="è²¼ä¸Šä¸­æ–‡è–ç¶“ç¶“æ–‡æˆ–è‹±æ–‡è¬›é“é€å­—ç¨¿..."
        )
        
        # è‡ªå‹•åµæ¸¬èªè¨€
        detected_lang = detect_language(input_text) if input_text else "unknown"
        
        col1, col2, col3 = st.columns([2, 2, 2])
        
        with col1:
            analysis_mode = st.radio(
                "åˆ†ææ¨¡å¼",
                ["è‡ªå‹•åµæ¸¬", "ä¸­æ–‡ç¶“æ–‡åˆ†æ (V1/V2)", "è‹±æ–‡è¬›ç¨¿åˆ†æ (Words/Phrases)"],
                index=0 if not input_text else (1 if detected_lang == "chinese" else 2)
            )
        
        with col2:
            if input_text:
                st.info(f"åµæ¸¬åˆ°ï¼š{'ä¸­æ–‡' if detected_lang == 'chinese' else 'è‹±æ–‡'}")
                st.caption(f"å­—æ•¸ï¼š{len(input_text)}")
        
        with col3:
            st.write("")
            st.write("")
            analyze_btn = st.button("ğŸ¤– é–‹å§‹ AI åˆ†æ", type="primary", use_container_width=True)
    
    # è³‡æ–™åº«æ“ä½œå€
    with st.expander("ğŸ—„ï¸ è³‡æ–™åº«ç®¡ç†", expanded=False):
        col1, col2, col3 = st.columns([2, 2, 2])
        
        with col1:
            search_ref = st.text_input("æœå°‹ Ref.", placeholder="ä¾‹ï¼š2Ti 4:17")
        with col2:
            search_kw = st.text_input("é—œéµå­—æœå°‹")
        with col3:
            st.write("")
            st.write("")
            if st.button("ğŸ” æœå°‹", use_container_width=True):
                # æœå°‹é‚è¼¯
                results = []
                for k, v in st.session_state.sentences.items():
                    match = False
                    if search_ref and search_ref.lower() in k.lower():
                        match = True
                    if search_kw and search_kw.lower() in str(v).lower():
                        match = True
                    if match:
                        results.append((k, v))
                
                if results:
                    st.session_state["search_results"] = results
                    st.success(f"æ‰¾åˆ° {len(results)} ç­†")
                else:
                    st.info("ç„¡ç¬¦åˆé …ç›®")
        
        # é¡¯ç¤ºæœå°‹çµæœ
        if "search_results" in st.session_state:
            for k, v in st.session_state["search_results"]:
                with st.container():
                    c1, c2 = st.columns([4, 1])
                    with c1:
                        st.markdown(f"**{k}**")
                        st.caption(v.get("en", "")[:100] + "...")
                    with c2:
                        if st.button("ğŸ—‘ï¸ åˆªé™¤", key=f"del_{k}"):
                            st.session_state.sentences.pop(k, None)
                            save_sentences()
                            st.rerun()
                    st.divider()
    
    # ============================================================
    # 4. åŸ·è¡Œåˆ†æ
    # ============================================================
    if analyze_btn and input_text:
        # æ±ºå®šåˆ†æé¡å‹
        if analysis_mode == "è‡ªå‹•åµæ¸¬":
            analysis_type = "chinese_verse" if detected_lang == "chinese" else "english_manuscript"
        elif "ä¸­æ–‡" in analysis_mode:
            analysis_type = "chinese_verse"
        else:
            analysis_type = "english_manuscript"
        
        with st.spinner(f"ğŸ¤– AI åˆ†æä¸­ï¼ˆ{ 'ä¸­æ–‡ç¶“æ–‡' if analysis_type == 'chinese_verse' else 'è‹±æ–‡è¬›ç¨¿' }ï¼‰..."):
            
            if api_key:
                result = analyze_with_ai(input_text, analysis_type, api_key)
            else:
                result = None
            
            # å¦‚æœ AI å¤±æ•—ï¼Œä½¿ç”¨å›é€€è³‡æ–™
            if result is None:
                result = create_fallback_data(input_text, analysis_type)
                st.warning("âš ï¸ ä½¿ç”¨é è¨­è³‡æ–™ï¼ˆè«‹æª¢æŸ¥ API Keyï¼‰")
            
            # å„²å­˜çµæœ
            st.session_state.current_analysis = result
            save_analysis_result(result, input_text, analysis_type)
            
            # å­˜å…¥è³‡æ–™åº«
            ref_no = result["ref_no"]
            st.session_state.sentences[ref_no] = {
                "ref": ref_no,
                "type": analysis_type,
                "en": result.get("ref_article", ""),
                "zh": result.get("ref_article_zh", ""),
                "date_added": dt.datetime.now().strftime("%Y-%m-%d %H:%M"),
                "data": result
            }
            save_sentences()
            
            st.session_state.show_result = True
            st.success(f"âœ… åˆ†æå®Œæˆï¼Ref: `{ref_no}`")
            st.rerun()
    
    # ============================================================
    # 5. é¡¯ç¤ºåˆ†æçµæœ
    # ============================================================
    if st.session_state.show_result and st.session_state.current_analysis:
        data = st.session_state.current_analysis
        
        st.divider()
        st.markdown(f"## ğŸ“‹ åˆ†æçµæœï¼š{data['ref_no']}")
        
        if data.get("is_fallback"):
            st.warning("âš ï¸ æ­¤ç‚ºé è¨­è³‡æ–™ï¼Œé AI åˆ†æçµæœ")
        
        # æ“ä½œæŒ‰éˆ•
        c1, c2, c3, c4 = st.columns([1.5, 1.5, 1.5, 1.5])
        with c1:
            if st.button("ğŸ“„ åˆ‡æ›åŸæ–‡é¡¯ç¤º"):
                st.session_state["show_article"] = not st.session_state.get("show_article", False)
                st.rerun()
        with c2:
            st.code(data['ref_no'])
        with c3:
            if st.button("ğŸ’¾ åŒ¯å‡º JSON"):
                st.download_button(
                    label="ä¸‹è¼‰ JSON",
                    data=json.dumps(data, ensure_ascii=False, indent=2),
                    file_name=f"{data['ref_no']}.json",
                    mime="application/json"
                )
        with c4:
            if st.button("ğŸ—‘ï¸ æ¸…é™¤çµæœ"):
                st.session_state.show_result = False
                st.session_state.current_analysis = None
                st.rerun()
        
        # é¡¯ç¤ºç²¾ç…‰æ–‡ç« 
        if st.session_state.get("show_article", False):
            with st.expander("ğŸ“˜ ç²¾ç…‰æ–‡ç« ", expanded=True):
                tabs = st.tabs(["è‹±æ–‡ç‰ˆ", "ä¸­è‹±å°ç…§"] if data.get("ref_article_zh") else ["æ–‡ç« "])
                
                with tabs[0]:
                    st.markdown(data.get("ref_article", "ç„¡è³‡æ–™"))
                
                if len(tabs) > 1 and data.get("ref_article_zh"):
                    with tabs[1]:
                        st.markdown(data["ref_article_zh"])
        
        # è³‡æ–™è¡¨æ ¼ï¼ˆä¾åˆ†æé¡å‹é¡¯ç¤ºä¸åŒæ¬„ä½ï¼‰
        if data.get("input_type") == "chinese_verse":
            # ä¸­æ–‡ç¶“æ–‡ï¼šé¡¯ç¤º V1/V2 æ ¼å¼
            v1_tab, v2_tab, words_tab, phrases_tab, grammar_tab = st.tabs([
                "ğŸ“Š V1 (è‹±ä¸­å°ç…§)", "ğŸ“Š V2 (æ—¥éŸ“æ³°)", "ğŸ“ å–®å­—", "ğŸ’¬ ç‰‡èª", "ğŸ“ æ–‡æ³•"
            ])
            
            with v1_tab:
                if "v1_data" in data:
                    v1 = data["v1_data"]
                    df = pd.DataFrame([v1])
                    st.dataframe(df, use_container_width=True)
                else:
                    # å¾ words/phrases/grammar çµ„åˆ V1 é¡¯ç¤º
                    st.info("V1 è³‡æ–™æ ¼å¼")
            
            with v2_tab:
                if "v2_data" in data:
                    v2 = data["v2_data"]
                    df = pd.DataFrame([v2])
                    st.dataframe(df, use_container_width=True)
                else:
                    st.info("V2 è³‡æ–™æ ¼å¼")
        else:
            # è‹±æ–‡è¬›ç¨¿ï¼šç›´æ¥é¡¯ç¤º Words/Phrases/Grammar
            words_tab, phrases_tab, grammar_tab = st.tabs(["ğŸ“ Words å–®å­—", "ğŸ’¬ Phrases ç‰‡èª", "ğŸ“ Grammar æ–‡æ³•"])
        
        # å–®å­—è¡¨
        with words_tab:
            words = data.get("words", [])
            if words:
                df = pd.DataFrame(words)
                # ç¢ºä¿æ¬„ä½é †åº
                cols = ["Vocab", "Syn_Ant", "Example", "å£èªè¨³", "KRF", "THSV11"]
                display_cols = [c for c in cols if c in df.columns]
                st.dataframe(df[display_cols], use_container_width=True, hide_index=True)
                st.caption(f"å…± {len(words)} å€‹å–®å­—")
            else:
                st.info("ç„¡å–®å­—è³‡æ–™")
        
        # ç‰‡èªè¡¨
        with phrases_tab:
            phrases = data.get("phrases", [])
            if phrases:
                df = pd.DataFrame(phrases)
                cols = ["Phrase", "Syn_Ant", "Example", "å£èªè¨³", "KRF", "THSV11"]
                display_cols = [c for c in cols if c in df.columns]
                st.dataframe(df[display_cols], use_container_width=True, hide_index=True)
                st.caption(f"å…± {len(phrases)} å€‹ç‰‡èª")
            else:
                st.info("ç„¡ç‰‡èªè³‡æ–™")
        
        # æ–‡æ³•è¡¨
        with grammar_tab:
            grammar = data.get("grammar", [])
            if grammar:
                df = pd.DataFrame(grammar)
                # æ–‡æ³•ç”¨ table é¡¯ç¤ºè¼ƒæ¸…æ¥š
                st.table(df)
                st.caption(f"å…± {len(grammar)} å€‹æ–‡æ³•é»")
            else:
                st.info("ç„¡æ–‡æ³•è³‡æ–™")
        
        # åŸå§‹è³‡æ–™ï¼ˆé™¤éŒ¯ç”¨ï¼‰
        with st.expander("ğŸ”§ åŸå§‹ JSON è³‡æ–™"):
            st.json(data)
    
    # ============================================================
    # 6. åŒ¯å‡ºèˆ‡ç®¡ç†
    # ============================================================
    st.divider()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        with st.expander("ğŸ“Š çµ±è¨ˆè³‡è¨Š"):
            total = len(st.session_state.sentences)
            st.metric("è³‡æ–™åº«ç­†æ•¸", total)
            
            types = {}
            for v in st.session_state.sentences.values():
                t = v.get("type", "unknown")
                types[t] = types.get(t, 0) + 1
            
            for t, c in types.items():
                st.caption(f"{t}: {c} ç­†")
    
    with col2:
        with st.expander("ğŸ“‹ åŒ¯å‡ºè³‡æ–™"):
            if st.button("åŒ¯å‡ºå…¨éƒ¨ (TSV)"):
                lines = ["Ref\tType\tEnglish\tChinese\tDate"]
                for k, v in st.session_state.sentences.items():
                    line = f"{k}\t{v.get('type','')}\t{v.get('en','')[:50]}\t{v.get('zh','')[:50]}\t{v.get('date_added','')}"
                    lines.append(line)
                st.code("\n".join(lines), language="text")
            
            if st.button("åŒ¯å‡º JSON"):
                st.code(json.dumps(st.session_state.sentences, ensure_ascii=False, indent=2)[:2000] + "...")
    
    with col3:
        with st.expander("âš ï¸ å±éšªæ“ä½œ"):
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºè³‡æ–™åº«", type="secondary"):
                confirm = st.checkbox("ç¢ºèªåˆªé™¤å…¨éƒ¨è³‡æ–™ï¼Ÿ")
                if confirm:
                    st.session_state.sentences = {}
                    save_sentences()
                    st.success("å·²æ¸…ç©ºè³‡æ–™åº«")
                    st.rerun()
