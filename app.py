import streamlit as st
import pandas as pd
import requests
import io
import re
import os
import base64

# --- 1. é é¢åŸºç¤é…ç½® ---
st.set_page_config(page_title="Memory Logic 2026", layout="wide", page_icon="ğŸ¶")

# --- 2. åˆå§‹åŒ– Session State ---
def init_session():
    if 'quiz_data' not in st.session_state:
        st.session_state.quiz_data = {"Text_CN": "å‡¡äº‹éƒ½æœ‰å®šæœŸï¼Œå¤©ä¸‹è¬å‹™éƒ½æœ‰å®šæ™‚ã€‚", "Text_EN": "To everything there is a season."}
    if 'verse_data' not in st.session_state:
        st.session_state.verse_data = {"Chinese": "å‡¡äº‹éƒ½æœ‰å®šæœŸï¼Œå¤©ä¸‹è¬å‹™éƒ½æœ‰å®šæ™‚ã€‚", "Reference": "å‚³é“æ›¸ 3:1", "Keyword": "å®šæ™‚"}
    if 'word_data' not in st.session_state:
        st.session_state.word_data = {"Vocab": "Study", "Definition": "å­¸ç¿’", "Grammar": "ä¿æŒå­¸ç¿’ï¼Œæ¯å¤©é€²æ­¥ï¼"}
    if 'phrase_data' not in st.session_state:
        st.session_state.phrase_data = {"Phrase": "Keep it up", "Definition": "ç¹¼çºŒåŠ æ²¹"}
    if 'score' not in st.session_state: st.session_state.score = 0

init_session()

THEME = {"bg": "#FFF9E3", "box": "#FFFFFF", "accent": "#FFCDD2", "text": "#4A4A4A", "sub": "#F06292", "keyword": "#E91E63"}

# --- 3. æ ¸å¿ƒå·¥å…·é¡åˆ¥ ---
class BibleAutomator:
    def __init__(self):
        # ä¿®æ­£ï¼šAPI å¿…é ˆåŒ…å«å”è­°
        self.api_base = "bible-api.com"
        self.analysis_keywords = ['Subject', 'Verb', 'è£œå…¨å¾Œ', 'ä¾‹å¥', 'è­¯ç‚º', 'æŒ‡ä»£', 'èªæ°£', 'çœç•¥', 'ä¸»è¬‚']
        self.lang_map = {"CN": "cuv", "EN": "web", "JA": "jpn", "KO": "kor", "TH": "tha"}

    def fetch_real_bible(self, ref, lang_key):
        """é€é API ç²å–ç¶“æ–‡è³‡æ–™"""
        trans = self.lang_map.get(lang_key, "web")
        try:
            # æ¸…ç†ç¶“æ–‡åƒè€ƒæ ¼å¼ï¼ˆç©ºæ ¼è½‰ç‚º +ï¼‰
            ref_encoded = ref.replace(" ", "+")
            r = requests.get(f"{self.api_base}{ref_encoded}?translation={trans}", timeout=7)
            if r.status_code == 200:
                return r.json().get('text', '').strip()
        except:
            pass
        return f"[ç„¡æ³•ç²å– {lang_key} ç‰ˆæœ¬]"

    def parse_manual(self, raw_text):
        """è§£ææ‰‹å‹•ç­†è¨˜ï¼ˆå·²ä¿®æ­£ Python re ä¸æ”¯æ´ \p{P} çš„å•é¡Œï¼‰"""
        book_match = re.search(r'([\u4e00-\u9fa5]+)(\d+)(ç¯‡|ç« )?', raw_text)
        book_name = book_match.group(1) if book_match else ""
        
        # ä»¥æ›è¡Œä¸”å¾Œæ–¹æ¥åº§æ¨™ (å¦‚ 19:1) ä½œç‚ºåˆ†å‰²é»
        blocks = re.split(r'\n(?=\d{1,3}:\d{1,3})', raw_text)
        
        final_list = []
        for block in blocks:
            lines = [l.strip() for l in block.split('\n') if l.strip()]
            if not lines: continue
            
            # ç¬¬ä¸€è¡Œé€šå¸¸æ˜¯åº§æ¨™
            ref_match = re.match(r'^(\d+:\d+)', lines[0])
            if not ref_match: continue
            
            ref_val = f"{book_name}{ref_match.group(1)}"
            entry = {
                "Reference": ref_val, "Chinese": "", "English": "", 
                "Key word": "", "Grammar": ""
            }
            
            grammar_lines = []
            for line in lines:
                # æ’é™¤åº§æ¨™è¡Œ
                if re.match(r'^\d+:\d+$', line): continue
                
                # åˆ¤å®šæ–‡æ³•è¡Œ
                if any(k in line for k in self.analysis_keywords):
                    grammar_lines.append(line)
                # åˆ¤å®šä¸­æ–‡è¡Œ
                elif re.search(r'[\u4e00-\u9fa5]', line) and not entry["Chinese"]:
                    entry["Chinese"] = line
                # åˆ¤å®šè‹±æ–‡è¡Œ (ä¿®æ­£å¾Œçš„ Regexï¼Œä¸ä½¿ç”¨ \p)
                elif re.match(r'^[A-Za-z0-9\s.,;!\?\'\"()\-\:]+$', line) and not entry["English"]:
                    entry["English"] = re.sub(r'^\d+:\d+\s*', '', line)
            
            entry["Grammar"] = "\n".join(grammar_lines)
            # æå–é•·å–®å­—ä½œç‚ºé—œéµå­—
            words = [w.strip('.,!?;') for w in entry["English"].split() if len(w) > 5]
            entry["Key word"] = ", ".join(list(dict.fromkeys(words))[:3])
            final_list.append(entry)
            
        return pd.DataFrame(final_list)

# --- 4. æ•ˆèƒ½å„ªåŒ–å…ƒä»¶ ---
@st.cache_data(ttl=600)
def get_img_base64(file_path):
    if os.path.exists(file_path):
        with open(file_path, "rb") as f:
            return f"data:image/jpeg;base64,{base64.b64encode(f.read()).decode()}"
    return None

# --- 5. CSS æ¨£å¼ ---
st.markdown(f"""
    <style>
    .stApp {{ background-color: {THEME['bg']}; }}
    .feature-box {{
        background: {THEME['box']}; border-radius: 15px; padding: 20px;
        border: 2px solid {THEME['accent']}; box-shadow: 4px 4px 0px {THEME['accent']};
        margin-bottom: 15px;
    }}
    .img-container img {{ border-radius: 12px; width: 100%; object-fit: cover; }}
    </style>
    """, unsafe_allow_html=True)

# --- 6. UI ä½ˆå±€ ---
tab_home, tab_play, tab_tool = st.tabs(["ğŸ  æˆ‘çš„æ›¸æ¡Œ", "ğŸ¯ éš¨è¨˜æŒ‘æˆ°", "ğŸ§ª è‡ªå‹•åŒ–å·¥å…·"])

with tab_home:
    v1, w1, p1 = st.session_state.verse_data, st.session_state.word_data, st.session_state.phrase_data
    col_main, col_side = st.columns([2, 1])
    
    with col_main:
        st.markdown(f'<div class="feature-box"><h3>ğŸ’¡ ä»Šæ—¥é‡‘å¥</h3><p style="font-size:20px;">{v1["Chinese"]}</p><div style="text-align:right;">â€” {v1["Reference"]}</div></div>', unsafe_allow_html=True)
        c1, c2 = st.columns(2)
        c1.markdown(f'<div class="feature-box"><small>ğŸ”¤ å–®å­—</small><br><b>{w1["Vocab"]}</b><br>{w1["Definition"]}</div>', unsafe_allow_html=True)
        c2.markdown(f'<div class="feature-box"><small>ğŸ”— ç‰‡èª</small><br><b>{p1["Phrase"]}</b><br>{p1["Definition"]}</div>', unsafe_allow_html=True)
        st.markdown(f'<div class="feature-box" style="background:#E3F2FD;"><small>ğŸ“ æ–‡æ³•ç­†è¨˜</small><br>{w1["Grammar"]}</div>', unsafe_allow_html=True)

    with col_side:
        img_data = get_img_base64("f364bd220887627.67cae1bd07457.jpg")
        if img_data: st.markdown(f'<div class="img-container"><img src="{img_data}"></div>', unsafe_allow_html=True)

with tab_play:
    st.subheader("ğŸ¯ ç¿»è­¯æŒ‘æˆ°")
    curr = st.session_state.quiz_data
    st.info(f"è«‹ç¿»è­¯ï¼š{curr['Text_CN']}")
    user_input = st.text_input("è¼¸å…¥è‹±æ–‡...")
    if st.button("æäº¤ç­”æ¡ˆ"):
        if user_input.lower().strip() == curr['Text_EN'].lower().strip():
            st.success("å¤ªæ£’äº†ï¼å®Œå…¨æ­£ç¢º")
            st.balloons()
        else:
            st.warning(f"å†è©¦ä¸€æ¬¡ï¼Ÿåƒè€ƒç­”æ¡ˆï¼š{curr['Text_EN']}")

with tab_tool:
    st.markdown("### ğŸ§ª è–ç¶“è³‡æ–™è§£æå·¥å…· (2026)")
    mode = st.radio("æ¨¡å¼é¸æ“‡", ["æ‰‹å‹•è§£æç­†è¨˜", "API è¯ç¶²æŠ“å–"], horizontal=True)
    auto = BibleAutomator()
    
    if mode == "æ‰‹å‹•è§£æç­†è¨˜":
        raw_text = st.text_area("è²¼ä¸Šå«åº§æ¨™çš„ç¶“æ–‡è§£æï¼š", height=200, placeholder="è©©ç¯‡19ç¯‡\n19:1 The heavens...\nSubject: The heavens")
        if st.button("ğŸš€ é–‹å§‹è§£æ"):
            if raw_text:
                df = auto.parse_manual(raw_text)
                st.dataframe(df, use_container_width=True)
                st.download_button("ğŸ“¥ åŒ¯å‡º Excel (CSV)", df.to_csv(index=False).encode('utf-8-sig'), "bible_notes.csv")
    
    else:
        ref_input = st.text_input("è¼¸å…¥ç¶“æ–‡åº§æ¨™ (ä¾‹å¦‚: Psalms 19:1)")
        if st.button("ğŸ” è¯ç¶²æŠ“å–"):
            with st.spinner("é€£ç·šä¸­..."):
                res = {
                    "ä¸­æ–‡": auto.fetch_real_bible(ref_input, "CN"),
                    "è‹±æ–‡": auto.fetch_real_bible(ref_input, "EN"),
                    "æ—¥æ–‡": auto.fetch_real_bible(ref_input, "JA")
                }
                st.json(res)
