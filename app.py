import streamlit as st
import pandas as pd
import requests
import io
import re
import random
import time
import json
from urllib.parse import quote

# --- 1. é é¢é…ç½®èˆ‡ä¸»é¡Œè¨­å®š (åƒ…ä¿ç•™å¥¶æ²¹é»ƒ) ---
st.set_page_config(page_title="Memory Logic 2026", layout="wide", page_icon="ğŸ¶")

THEME = {"bg": "#FFF9E3", "box": "#FFFFFF", "accent": "#FFCDD2", "text": "#4A4A4A", "sub": "#F06292", "keyword": "#E91E63"}

# --- Lottie å‹•ç•«åŠ è¼‰å‡½æ•¸ ---
def load_lottieurl(url):
    try:
        r = requests.get(url, timeout=5)
        if r.status_code != 200:
            return None
        return r.json()
    except requests.exceptions.RequestException:
        return None

# Lottie URLs for Mario and Snoopy (Example from Lottiefiles.com)
LOTTIE_MARIO = "assets9.lottiefiles.com"
LOTTIE_SNOOPY = "assets7.lottiefiles.com"

# --- 2. å´é‚Šæ¬„æ§åˆ¶å° ---
with st.sidebar:
    st.markdown("### ğŸ¾ ç³»çµ±æ§åˆ¶å°")
    st.markdown(f"**ç•¶å‰ä¸»é¡Œï¼šSnoopy Retro (å¥¶æ²¹é»ƒ)**")
    
    st.divider()
    if 'score' not in st.session_state: st.session_state.score = 0
    if 'lives' not in st.session_state: st.session_state.lives = 3
    if 'count' not in st.session_state: st.session_state.count = 0
    
    st.subheader(f"ğŸ† ç´¯è¨ˆå¾—åˆ†: {st.session_state.score}")
    st.subheader(f"â¤ï¸ å‰©é¤˜ç”Ÿå‘½: {'â¤ï¸' * st.session_state.lives}")
    
    st.progress(min(st.session_state.count / 20.0, 1.0))
    st.caption(f"ä»Šæ—¥å­¸ç¿’é€²åº¦: {st.session_state.count}/20")

    if st.button("â™»ï¸ åˆ·æ–°å…§å®¹ä¸¦åŒæ­¥"):
        st.cache_data.clear()
        st.rerun()

# --- 3. CSS æ³¨å…¥ (åƒ…ä¿ç•™å¿…è¦æ¨£å¼) ---
st.markdown(f"""
    <style>
    @import url('fonts.googleapis.com');
    html, body, [data-testid="stAppViewContainer"] {{ background-color: {THEME['bg']}; font-family: 'Comic Neue', cursive; }}
    
    .feature-box {{
        background-color: {THEME['box']} !important;
        border-radius: 18px !important;
        padding: 20px !important;
        border: 3px solid {THEME['accent']} !important;
        box-shadow: 6px 6px 0px {THEME['accent']} !important;
        margin-bottom: 15px !important;
        min-height: 120px; /* ç¢ºä¿å°æ¡†æ¡†æœ‰åŸºæœ¬é«˜åº¦ */
    }}
    
    .typing {{
        overflow: hidden; border-right: .15em solid orange; white-space: nowrap;
        animation: typing 3s steps(40, end), blink-caret .75s step-end infinite;
    }}
    @keyframes typing {{ from {{ width: 0 }} to {{ width: 100% }} }}
    @keyframes blink-caret {{ from, to {{ border-color: transparent }} 50% {{ border-color: orange; }} }}

    .dict-btn {{ color: {THEME['sub']}; text-decoration: none; font-weight: bold; float: right; font-size: 12px; }}
    .kw {{ color: {THEME['keyword']}; font-weight: bolder; font-size: 26px; }} /* é—œéµå­—æ¨£å¼ */
    </style>
    """, unsafe_allow_html=True)

# --- 4. è³‡æ–™åº«æŠ“å–èˆ‡åˆ†é¡é‚è¼¯ ---
SHEET_ID = "1eiinJgMYXkCwIbU25P7lfsyNhO8MtD-m15wyUv3YgjQ"
GIDS = {"ğŸ“– ç¶“ç¯€": "1454083804", "ğŸ”¤ å–®å­—": "1400979824", "ğŸ”— ç‰‡èª": "1657258260"}

@st.cache_data(ttl=300)
def fetch_data(gid):
    url = f"docs.google.com{SHEET_ID}/export?format=csv&gid={gid}"
    try:
        r = requests.get(url, timeout=10)
        return pd.read_csv(io.StringIO(r.text)).fillna("")
    except: return pd.DataFrame()

def heuristic_classify(item):
    item = item.strip()
    if re.search(r'\b\d{1,3}:\d{1,3}\b', item): return "Verses"
    tokens = item.split()
    if len(tokens) <= 1: return "Words"
    if 2 <= len(tokens) <= 6: return "Phrases"
    return "Verses"

# --- 5. ä¸»åˆ†é æ¶æ§‹ ---
tab_home, tab_play, tab_tool = st.tabs(["ğŸ  æˆ‘çš„æ›¸æ¡Œ", "ğŸ¯ éš¨æ©ŸæŒ‘æˆ°", "ğŸ§ª è‡ªå‹•åˆ†é¡å·¥å…·"])

with tab_home:
    df_v = fetch_data(GIDS["ğŸ“– ç¶“ç¯€"])
    df_w = fetch_data(GIDS["ğŸ”¤ å–®å­—"])
    df_p = fetch_data(GIDS["ğŸ”— ç‰‡èª"])

    v1 = df_v.sample(1).iloc[0] if not df_v.empty else {"Chinese": "è¼‰å…¥ä¸­...", "Reference": "", "Keyword": ""}
    w1 = df_w.sample(1).iloc[0] if not df_w.empty else {"Vocab": "Study", "Definition": "å­¸ç¿’", "Grammar": ""}
    p1 = df_p.sample(1).iloc[0] if not df_p.empty else {"Phrase": "Keep it up", "Definition": "ç¹¼çºŒåŠ æ²¹", "Grammar": ""}

    # 5.1 é¡¯ç¤ºå²åŠªæ¯”åœ–ç‰‡ (å‡è¨­åœ–ç‰‡å·²åœ¨ GitHub repo æ ¹ç›®éŒ„)
    st.markdown('<div style="text-align:center;">', unsafe_allow_html=True)
    img_files = ["f364bd220887627.67cae1bd07457.jpg", "183ebb183330643.Y3JvcCw4MDgsNjMyLDAsMA.jpg", "68254faebaafed9dafb41918f74c202e.jpg"]
    for img_name in img_files:
        if os.path.exists(img_name):
            st.image(img_name, width=150)
    st.markdown('</div>', unsafe_allow_html=True)


    # 5.2 ä¸Šæ–¹å¤§æ¡† (ç¶“ç¯€ + é—œéµå­—æ¨™è‰²)
    chinese_text = v1["Chinese"]
    keyword = v1.get("Keyword", "").strip()
    if keyword and keyword in chinese_text:
        marked_text = chinese_text.replace(keyword, f'<span class="kw">{keyword}</span>')
    else:
        marked_text = chinese_text
        
    st.markdown(f'<div class="feature-box"><h3 style="color:{THEME["sub"]};">ğŸ’¡ ä»Šæ—¥é‡‘å¥</h3><div class="typing" style="font-size:24px;">â€œ{marked_text}â€</div><div style="color:gray; margin-top:10px;">â€” {v1["Reference"]}</div></div>', unsafe_allow_html=True)

    # 5.3 ä¸‹æ–¹ä¸‰æ¬„ (å­—å…¸é€£çµ, ç‰‡èª/æ–‡æ³•é€£çµ)
    c1, c2, c3 = st.columns([1, 1, 2])
    with c1:
        vocab = w1["Vocab"]
        dict_url = f"dictionary.cambridge.org{quote(vocab)}"
        st.markdown(f'<div class="feature-box"><a href="{dict_url}" target="_blank" class="dict-btn">ğŸ” å­—å…¸</a><b style="color:{THEME["sub"]};">ğŸ”¤ å–®å­—</b><br><span style="font-size:20px;">{vocab}</span><br><small>{w1["Definition"]}</small></div>', unsafe_allow_html=True)
    with c2:
        phrase = p1["Phrase"]
        grammar_search_url = f"www.google.com{quote(phrase + ' Grammar usage')}"
        st.markdown(f'<div class="feature-box"><a href="{grammar_search_url}" target="_blank" class="dict-btn">ğŸ”— åƒè€ƒ</a><b style="color:{THEME["sub"]};">ğŸ”— ç‰‡èª</b><br><span style="font-size:18px;">{phrase}</span><br><small>{p1["Definition"]}</small></div>', unsafe_allow_html=True)
    with c3:
        gram = w1.get("Grammar") if pd.notna(w1.get("Grammar")) else "ä¿æŒå­¸ç¿’ï¼"
        st.markdown(f'<div class="feature-box" style="background-color:#E3F2FD !important;"><b style="color:#0288D1;">ğŸ“ é—œéµæ–‡æ³•</b><br><p style="font-size:16px;">{gram}</p></div>', unsafe_allow_html=True)

# --- TAB 2: é—–é—œæŒ‘æˆ° (Lottie å‹•ç•«éŠæˆ²) ---
with tab_play:
    # ä½¿ç”¨ Lottie å‹•ç•«ä»£æ›¿æ–‡å­—åœ–ç¤º
    col_lottie1, col_lottie2, col_lottie3 = st.columns([1, 1, 1])
    with col_lottie1:
        if st.session_state.lives > 0:
            st_lottie(load_lottieurl(LOTTIE_MARIO), height=100, key="mario_run")
    with col_lottie3:
        st_lottie(load_lottieurl(LOTTIE_SNOOPY), height=100, key="snoopy_dance")

    if st.session_state.lives <= 0:
        st.error("ğŸ’€ GAME OVER! é¦¬åˆ©æ­éœ€è¦ä¼‘æ¯...")
        if st.button("ä½¿ç”¨ 1UP è˜‘è‡é‡ç”Ÿ"):
            st.session_state.lives = 3
            st.rerun()
    else:
        st.subheader("âš¡ï¸ ç¬æ™‚ç¿»è­¯æŒ‘æˆ° (ä¸­ç¿»è‹±)")
        q_item = w1 if random.random() > 0.5 else p1
        target = q_item.get("Vocab") or q_item.get("Phrase", "")
        st.write(f"é¡Œç›®ï¼š ã€Œ **{q_item.get('Definition')}** ã€ çš„æ­£ç¢ºç¿»è­¯æ˜¯ï¼Ÿ")
        ans = st.text_input("åœ¨æ­¤è¼¸å…¥ç­”æ¡ˆ...", key="game_input")
        if st.button("æäº¤ç­”æ¡ˆ"):
            if ans.lower().strip() == str(target).lower().strip():
                st.balloons()
                st.session_state.score += 10
                st.session_state.count += 1
                st.success("âœ… æ­£ç¢ºï¼")
                time.sleep(1)
                st.rerun()
            else:
                st.session_state.lives -= 1
                st.error(f"âŒ ç­”éŒ¯äº†ï¼ç”Ÿå‘½å€¼ -1ã€‚")

# --- TAB 3: è‡ªå‹•åˆ†é¡å·¥å…· ---
with tab_tool:
    st.markdown("### ğŸ§ª AI è‡ªå‹•åˆ†é¡èˆ‡å°å‡º")
    input_text = st.text_area("åœ¨æ­¤è²¼ä¸Šæ–‡ç« ...", height=200)
    if st.button("ğŸš€ é–‹å§‹åˆ†æåˆ†é¡"):
        lines = re.split(r'\n+|(?<=[ã€‚ï¼ï¼Ÿ\.\?\!;ï¼›])\s*', input_text)
        results = [{"å…§å®¹": l.strip(), "å»ºè­°åˆ†é¡": heuristic_classify(l)} for l in lines if l.strip()]
        if results:
            st.dataframe(pd.DataFrame(results), use_container_width=True)
            # Add download button functionality here if needed
